---
title: CUDA Tiled Matrix Multiplication
description: Matrix Multiplication in CUDA, massively optimized by tiling, shared memory, avoiding bank conflicts, and warps.
authors: yuxiao
tags: [techniques, cuda]
---

Matrix Multiplication in CUDA, including tiling, shared memory, resolving bank conflicts, and warps.

Problem Statement: Given two matrix M and N, compute a result matrix P=MxN. Implement the computation in CUDA.

In this blog, we will focus on the computation, so we assume we have got two matrix M and N allocated on device, and the result matrix is also allocated on device. For simplication, we will not copy it back to the host.

<!-- truncate -->

### Part 0 Struct Declaraton

```cpp
// Matrix Structure declaration
typedef struct {
    unsigned int width;
    unsigned int height;
    unsigned int pitch;
    float* elements;
} Matrix;
```

### Part 1 Naive Solution

Let's start with a naive solution. We separate the kernel from the computation and store it in another file, in which we have a kernel function like this `__global__ void MatrixMulKernel(Matrix M, Matrix N, Matrix P)`. The `__global__` declaration specifier makes the function callable from the host directly. The execution of the kernel function is on the GPU device.

```cpp
__global__ void MatrixMulKernel(Matrix M, Matrix N, Matrix P)
{
	// Multiply the two matrices
	/* Version 1: Single-thread */
	for (unsigned int i = 0; i < M.height; i++) {
		for (unsigned int j = 0; j < N.width; j++) {
			float sum = 0;
			for (unsigned int k = 0; k < M.width; k++) {
				sum += M.elements[i * M.width + k] * N.elements[k * N.width + j];
			}
			P.elements[i * N.width + j] = sum;
		}
	}
	
	/* Version 2: One block with multiple threads */
	float sum = 0;
	for (unsigned int k = 0; k < M.width; k++) {
		sum += M.elements[threadIdx.x * M.width + k] * N.elements[k * N.width + threadIdx.y];
	}
	P.elements[threadIdx.x * N.width + threadIdx.y] = sum;
}
```

To call the kernel function from the host, we need to pass the kernel arguments, in which the dimensions of the grid and block are specified.

```cpp
void MatrixMulOnDevice(const Matrix M, const Matrix N, Matrix P)
{
	/* Version 1: Single-thread */
	MatrixMulKernel<<<1, 1>>>(M, N, P);
    cudaDeviceSynchronize();
	/* Version 2: One block with multiple threads */
	dim3 threadBlock(M.height, N.width);
	MatrixMulKernel<<<1, threadBlock>>>(M, N, P);
    cudaDeviceSynchronize();
}
```

Now here is the explanation. Version 1 is pretty straight forward. It simply does a vanilla matrix multiplication in 1 CUDA thread. The same code should work on the CPU as well. In version 2, we fill one block with `M.height * N.width` threads. Each thread computes an element of the result matrix. Since we used the `M.height` as the first dimension and `N.width` as the second dimension, the `threadIdx.x` corresponds to the height of `P` and `threadIdx.y` corresponds to the width of `P`. 

Note that calling a kernel function is **asynchronous**! We need to explicitly synchronize with the GPU device by calling `cudaDeviceSynchronize`.

**Limitation of version 1**: Too slow! Why buy a GPU if you use it like a CPU?

**Limitation of version 2**: Too narrow! One block can container no more than 1024 threads, so the size of P is limited to `<=1024`.

### Part 2 Memory Coalesce and Bank Conflicts

To fully make use of the GPU device (you spend money it!), we will use a technique called **Tiling**. 

To simplify, let's say the tile size equals to the block size. If a thread block has 32x32 threads, then each tile is 32x32 as well, so that every block is responsible for one tile while every thread is responsible for one tile unit. Let's modify our `MatrixMulOnDevice` as follows:

```cpp
void MatrixMulOnDevice(const Matrix M, const Matrix N, Matrix P)
{
	// Kernel configuration
	// ROUND_UP(P.width / TILE_SIZE) * ROUND_UP(P.height / TILE_SIZE) blocks, 1024 threads each, threadIdx.x -- width, threadIdx.y -- height
	const unsigned int TILE_SIZE = 32;
	dim3 gridBlocks((P.width + TILE_SIZE - 1) / TILE_SIZE, (P.height + TILE_SIZE - 1) / TILE_SIZE);
	dim3 blockThreads(TILE_SIZE, TILE_SIZE);

	// Launch the kernel
	MatrixMulKernel<<<gridBlocks, blockThreads>>>(M, N, P);
	cudaDeviceSynchronize();
}
```

In order to coalesce memory access, threads in the same block will walk through a tile of the matrix M and N, and accumulate the partial result in a thread-private variable `sum`. Tile `A` is from M and Tile `B` is from N. In the code below, we coalesce the memory access of threads. But the question is: how and why? To answer that question, let's introduce the concept **Warp** first. Warp is the **minimum scheduling unit** in the Turing Architecture, instead of the thread. Each warp can have up to 32 active threads and the maximum warps per block is 32 (due to the 1,024 threads/block limit). Threads in a warp have the **same `threadIdx.y` but different `threadIdx.x`**. That means, if you start a kernel with the block dimension set to `(16,8)`, then the GPU will create 8 warps, and each warp has 16 active threads and 16 inactive threads so that there is exactly 32 threads in a warp. What's more, the **`threadIdx.x`** of threads in a warp is consecutive. Now let's check the memory access pattern. We know that in the Turing architecture, global memory access is done in transactions. In each transaction, one warp gets a 128-byte memory chunk. If the memory that each thread requests is consecutive overall, then we are good. As the `threadIdx.x` is consecutive, both `A[threadIdx.y][threadIdx.x]` and `M.elements[(row * M.width + TILE_SIZE * i + threadIdx.x)]` are consecutive as well. The same thing applies to tile `B` and matrix `N`. The threads in a warp are requesting memory in 128-byte chunks. Note that we need to synchronize all threads after the memory access, so that in the computation stage, the data in tile `A` and `B` is as expected by all threads. The shared memory access in the computation stage is free of bank conflicts. This is because in each warp, `k` and `threadIdx.y` are the same across active threads, and `threadIdx.x` is consecutive. After the computation, we need to do one more synchronization of threads to prevent the situation in which a fast thread updates A and B with the next tile and a slow thread has not finished computing the current tile. Finally, each thread update one element of `P` with the result, which is also memory coalesced because of the same `row` and consecutive `col`. Here is the code:

```cpp
__global__ void MatrixMulKernel(Matrix M, Matrix N, Matrix P)
{
	// Set tile size = block size
	const unsigned int TILE_SIZE = 32;
	// Round up tiles
	const unsigned int num_tile = (M.width + TILE_SIZE - 1) / TILE_SIZE;
	// Load a tile, shared among threads in a block
	__shared__ float A[TILE_SIZE][TILE_SIZE];
	__shared__ float B[TILE_SIZE][TILE_SIZE];
    // Partial sum
	float sum = 0;
	// The row and col of P
	unsigned int row = blockIdx.y * TILE_SIZE + threadIdx.y;	// assert(TILE_SIZE == BLOCK_SIZE)
	unsigned int col = blockIdx.x * TILE_SIZE + threadIdx.x;	// assert(TILE_SIZE == BLOCK_SIZE)

	for (unsigned int i = 0; i < num_tile; i++) {
		if (row < M.height && TILE_SIZE * i + threadIdx.x < M.width)
			A[threadIdx.y][threadIdx.x] = M.elements[(row * M.width + TILE_SIZE * i + threadIdx.x)];
		else
			A[threadIdx.y][threadIdx.x] = 0;
		if (TILE_SIZE * i + threadIdx.y < N.height && col < N.width)
			B[threadIdx.y][threadIdx.x] = N.elements[((TILE_SIZE * i + threadIdx.y) * N.width + col)];
		else
			B[threadIdx.y][threadIdx.x] = 0;
        
		// Alternative implementation, still uses branch, no big difference
		// A[threadIdx.y][threadIdx.x] = (row < M.height && TILE_SIZE * i + threadIdx.x < M.width) ? M.elements[(row * M.width + TILE_SIZE * i + threadIdx.x)] : 0;
		// B[threadIdx.y][threadIdx.x] = (TILE_SIZE * i + threadIdx.y < N.height && col < N.width) ? N.elements[((TILE_SIZE * i + threadIdx.y) * N.width + col)] : 0;

		// sync, memory barrier, ensure A and B are complete before the computation of each thread
		__syncthreads();
        
        // Compute one tile, A*B
		// Equivalent to P[row][col] += A[threadIdx.y][k] * B[k][threadIdx.x], but we use a private var sum to store it, and assign it later to reduce memory access
		for (unsigned int k = 0; k < TILE_SIZE; k++) {
			sum += A[threadIdx.y][k] * B[k][threadIdx.x];
		}

		// Must sync before the next iteration to ensure A and B are correct
		__syncthreads();
	}
    
	// write back
	if (row < P.height && col < P.width)
		P.elements[row * P.width + col] = sum;
}
```

### Reflections

Now we have some quick questions.

*Question 1*: Can I have my `for` loop implemented like this?

```cpp
	for (unsigned int i = 0; i < num_tile; i++) {
		if (row < M.height && TILE_SIZE * i + threadIdx.x < M.width)
			A[threadIdx.x][threadIdx.y] = M.elements[(row * M.width + TILE_SIZE * i + threadIdx.x)];
		else
			A[threadIdx.x][threadIdx.y] = 0;
		if (TILE_SIZE * i + threadIdx.y < N.height && col < N.width)
			B[threadIdx.x][threadIdx.y] = N.elements[((TILE_SIZE * i + threadIdx.y) * N.width + col)];
		else
			B[threadIdx.x][threadIdx.y] = 0;

		// sync, memory barrier, ensure A and B are complete before the computation of each thread
		__syncthreads();
        
        // Compute one tile, A*B
		// Equivalent to P[row][col] += A[k][threadIdx.y] * B[threadIdx.x][k], but we use a private var sum to store it, and assign it later to reduce memory access
		for (unsigned int k = 0; k < TILE_SIZE; k++) {
			sum += A[k][threadIdx.y] * B[threadIdx.x][k];
		}

		// Must sync before the next iteration to ensure A and B are correct
		__syncthreads();
	}

```

The answer is yes. This implementation is correct. However, it will cause a huge number of bank conflicts. The reason is, the memory layout of tile `A` and `B` is row-first. And because of the bank is 4-byte aligned, different threads in a warp can fall into the same bank since they differ in `threadIdx.x` rather than `threadIdx.y`.

*Question 2*: Can we optimize the code so that it is free of branch divergence? The answer is also yes, but with some trade-off. An alternative implementation is:

```cpp
		// more registers, less if else
		A[threadIdx.y][threadIdx.x] = M.elements[(row * M.width + TILE_SIZE * i + threadIdx.x) % (M.height * M.width)] * (row < M.height && TILE_SIZE * i + threadIdx.x < M.width);
		B[threadIdx.y][threadIdx.x] = N.elements[((TILE_SIZE * i + threadIdx.y) * N.width + col) % (N.height * N.width)] * (TILE_SIZE * i + threadIdx.y < N.height && col < N.width);
```

This will cause the compiler to add more registers to each thread. If the number of registers that each thread uses becomes the bottleneck of the scheduler, we should choose another one.

### Profiling

In this section, we will demonstrate the profiling results of the above implementations by using the `ncu` profiling tool.

First, check what metrics are support for the GPU device:

```bash
ncu --query-metrics | grep -i shared
```

In my experiment machine, the metrics `l1tex__data_bank_conflicts_pipe_lsu_mem_global` and `l1tex__data_bank_conflicts_pipe_lsu_mem_shared` are what I am looking for. So I will then run:

```bash
ncu -f --metrics l1tex__data_bank_conflicts_pipe_lsu_mem_global,l1tex__data_bank_conflicts_pipe_lsu_mem_shared -o profile ./program
```

to generate the profiling results. Finally, we use `ncu --import profile.ncu-rep --csv` to interpret the results in human-readable formats.

**Results for the correct implementation**

```
"ID","Process ID","Process Name","Host Name","Kernel Name","Context","Stream","Block Size","Grid Size","Device","CC","Section Name","Metric Name","Metric Unit","Metric Value"
"0","1495073","lab2","127.0.0.1","MatrixMulKernel(Matrix, Matrix, Matrix)","1","7","(32, 32, 1)","(11, 20, 1)","0","7.5","Command line profiler metrics","l1tex__data_bank_conflicts_pipe_lsu_mem_global.avg","","0"
"0","1495073","lab2","127.0.0.1","MatrixMulKernel(Matrix, Matrix, Matrix)","1","7","(32, 32, 1)","(11, 20, 1)","0","7.5","Command line profiler metrics","l1tex__data_bank_conflicts_pipe_lsu_mem_global.max","","0"
"0","1495073","lab2","127.0.0.1","MatrixMulKernel(Matrix, Matrix, Matrix)","1","7","(32, 32, 1)","(11, 20, 1)","0","7.5","Command line profiler metrics","l1tex__data_bank_conflicts_pipe_lsu_mem_global.min","","0"
"0","1495073","lab2","127.0.0.1","MatrixMulKernel(Matrix, Matrix, Matrix)","1","7","(32, 32, 1)","(11, 20, 1)","0","7.5","Command line profiler metrics","l1tex__data_bank_conflicts_pipe_lsu_mem_global.sum","","0"
"0","1495073","lab2","127.0.0.1","MatrixMulKernel(Matrix, Matrix, Matrix)","1","7","(32, 32, 1)","(11, 20, 1)","0","7.5","Command line profiler metrics","l1tex__data_bank_conflicts_pipe_lsu_mem_shared.avg","","0"
"0","1495073","lab2","127.0.0.1","MatrixMulKernel(Matrix, Matrix, Matrix)","1","7","(32, 32, 1)","(11, 20, 1)","0","7.5","Command line profiler metrics","l1tex__data_bank_conflicts_pipe_lsu_mem_shared.max","","0"
"0","1495073","lab2","127.0.0.1","MatrixMulKernel(Matrix, Matrix, Matrix)","1","7","(32, 32, 1)","(11, 20, 1)","0","7.5","Command line profiler metrics","l1tex__data_bank_conflicts_pipe_lsu_mem_shared.min","","0"
"0","1495073","lab2","127.0.0.1","MatrixMulKernel(Matrix, Matrix, Matrix)","1","7","(32, 32, 1)","(11, 20, 1)","0","7.5","Command line profiler metrics","l1tex__data_bank_conflicts_pipe_lsu_mem_shared.sum","","0"
```

0 bank conflict, perfect!

**Results for the implementation in Q1**

```
"ID","Process ID","Process Name","Host Name","Kernel Name","Context","Stream","Block Size","Grid Size","Device","CC","Section Name","Metric Name","Metric Unit","Metric Value"
"0","1496073","lab2","127.0.0.1","MatrixMulKernel(Matrix, Matrix, Matrix)","1","7","(32, 32, 1)","(11, 20, 1)","0","7.5","Command line profiler metrics","l1tex__data_bank_conflicts_pipe_lsu_mem_global.avg","","0"
"0","1496073","lab2","127.0.0.1","MatrixMulKernel(Matrix, Matrix, Matrix)","1","7","(32, 32, 1)","(11, 20, 1)","0","7.5","Command line profiler metrics","l1tex__data_bank_conflicts_pipe_lsu_mem_global.max","","0"
"0","1496073","lab2","127.0.0.1","MatrixMulKernel(Matrix, Matrix, Matrix)","1","7","(32, 32, 1)","(11, 20, 1)","0","7.5","Command line profiler metrics","l1tex__data_bank_conflicts_pipe_lsu_mem_global.min","","0"
"0","1496073","lab2","127.0.0.1","MatrixMulKernel(Matrix, Matrix, Matrix)","1","7","(32, 32, 1)","(11, 20, 1)","0","7.5","Command line profiler metrics","l1tex__data_bank_conflicts_pipe_lsu_mem_global.sum","","0"
"0","1496073","lab2","127.0.0.1","MatrixMulKernel(Matrix, Matrix, Matrix)","1","7","(32, 32, 1)","(11, 20, 1)","0","7.5","Command line profiler metrics","l1tex__data_bank_conflicts_pipe_lsu_mem_shared.avg","","829,063.53"
"0","1496073","lab2","127.0.0.1","MatrixMulKernel(Matrix, Matrix, Matrix)","1","7","(32, 32, 1)","(11, 20, 1)","0","7.5","Command line profiler metrics","l1tex__data_bank_conflicts_pipe_lsu_mem_shared.max","","896,896"
"0","1496073","lab2","127.0.0.1","MatrixMulKernel(Matrix, Matrix, Matrix)","1","7","(32, 32, 1)","(11, 20, 1)","0","7.5","Command line profiler metrics","l1tex__data_bank_conflicts_pipe_lsu_mem_shared.min","","768,768"
"0","1496073","lab2","127.0.0.1","MatrixMulKernel(Matrix, Matrix, Matrix)","1","7","(32, 32, 1)","(11, 20, 1)","0","7.5","Command line profiler metrics","l1tex__data_bank_conflicts_pipe_lsu_mem_shared.sum","","28,188,160"
```

Too many bank conflicts!

Second, let's check if there are more registers used in Q2's implementation by:

```bash
nvcc -c -gencode=arch=compute_75,code='"sm_75,compute_75"' --ptxas-options=-v -o /dev/null ./kernel_source_code.cu
```

**Results for the correct implementation**

```
ptxas info    : 0 bytes gmem
ptxas info    : Compiling entry function '_Z15MatrixMulKernel6MatrixS_S_' for 'sm_75'
ptxas info    : Function properties for _Z15MatrixMulKernel6MatrixS_S_
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, used 1 barriers, 8192 bytes smem, 424 bytes cmem[0]
```

**Results for the implementation in Q2**

```
ptxas info    : 0 bytes gmem
ptxas info    : Compiling entry function '_Z15MatrixMulKernel6MatrixS_S_' for 'sm_75'
ptxas info    : Function properties for _Z15MatrixMulKernel6MatrixS_S_
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 46 registers, used 1 barriers, 8192 bytes smem, 424 bytes cmem[0]
```

We can see 6 more registers are used. This is not a small number. Each Streaming Multiprocessor (SM) on my GPU device only has 65,536 registers, while each block of this implementation has 1,024 threads. So the number of thread that can simultaneously run in a SM is 1,638 for 40 registers/thread and 1,424 for 46 registers/thread. If a block uses more that 65,536 registers, then it cannot be scheduled by an SM, we are cooked.
